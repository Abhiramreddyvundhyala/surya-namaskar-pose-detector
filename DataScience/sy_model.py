# -*- coding: utf-8 -*-
"""sy_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfPuN1yCRi6mV73fMLxqINlzRZNcJqGO
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

combined_csv = "/content/combined_dataset.csv"
df = pd.read_csv(combined_csv)

sequence_length = 40  # number of consecutive frames per sequence
print(f"✅ Loaded dataset: {df.shape}")
print(df.isna().sum())

X = df.drop(columns=["pose"]).values
y = df["pose"].values

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_seq, y_seq = [], []
for i in range(len(X) - sequence_length):
    X_seq.append(X[i:i+sequence_length])
    y_seq.append(y_encoded[i+sequence_length-1])

X_seq = np.array(X_seq)
y_seq = np.array(y_seq)
print(f"✅ Sequences: X_seq={X_seq.shape}, y_seq={y_seq.shape}")

X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_seq, test_size=0.2, random_state=42
)
print(f"Train: {X_train.shape}, Test: {X_test.shape}")

# Normalize features
mean = X_train.mean(axis=(0, 1), keepdims=True)
std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8
X_train = (X_train - mean) / std
X_test = (X_test - mean) / std

# ================================
# STEP 4: BUILD CNN + LSTM MODEL
# ================================
num_classes = len(label_encoder.classes_)

model = Sequential([
    # ---- CNN BLOCK 1 ----
    Conv1D(128, 3, padding='same', activation='relu',
           input_shape=(X_train.shape[1], X_train.shape[2])),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(0.25),

    # ---- CNN BLOCK 2 ----
    Conv1D(256, 3, padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(0.25),

    # ---- LSTM BLOCK ----
    LSTM(128, return_sequences=True),
    LSTM(64),
    Dropout(0.3),

    # ---- DENSE HEAD ----
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
model.summary()

# ================================
# STEP 5: TRAINING SETUP
# ================================
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau # Import ReduceLROnPlateau and ModelCheckpoint

early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
checkpoint = ModelCheckpoint('best_cnn_lstm.h5', monitor='val_loss', save_best_only=True)


history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=40,
    batch_size=32,
    callbacks=[early_stop, reduce_lr, checkpoint],
    verbose=1
)

# ================================
# STEP 6: EVALUATION
# ================================
loss, acc = model.evaluate(X_test, y_test)
print(f"✅ Test Accuracy: {acc:.2f}")

# ================================
# STEP 7: LEARNING CURVES
# ================================
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()
plt.show()

# ================================
# STEP 8: CONFUSION MATRIX + REPORT
# ================================
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

y_test_labels = label_encoder.inverse_transform(y_test)
y_pred_labels = label_encoder.inverse_transform(y_pred)

cm = confusion_matrix(y_test_labels, y_pred_labels, labels=label_encoder.classes_)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print("\nClassification Report:\n")
print(classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_))

